{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9/Zld2U9hM8Nc/L7vcpf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthchafle10/DLBootcamp-CV-25/blob/main/CV_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # should return True"
      ],
      "metadata": {
        "id": "awaXR0cosCVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to GPU for faster computational performance."
      ],
      "metadata": {
        "id": "JpUY-o9CsEY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c1bu84aptz4"
      },
      "outputs": [],
      "source": [
        "#Custom Dataset class (VOCDataset)\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# ==============================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================\n",
        "random.seed(42)\n",
        "TARGET_SIZE = 1200\n",
        "TRAIN_SPLIT = 0.75\n",
        "VAL_SPLIT   = 0.125\n",
        "TEST_SPLIT  = 0.125\n",
        "DATA_DIR = \"data_voc_subset\"\n",
        "VOC_ROOT = \"VOCdevkit/VOC2012\"\n",
        "\n",
        "# ==============================================================\n",
        "# TRY DOWNLOADING PASCAL VOC 2012\n",
        "# ==============================================================\n",
        "def download_voc():\n",
        "    try:\n",
        "        if not os.path.exists(VOC_ROOT):\n",
        "            url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "            filename = \"VOCtrainval_11-May-2012.tar\"\n",
        "            if not os.path.exists(filename):\n",
        "                print(\"üì• Downloading PASCAL VOC 2012...\")\n",
        "                urllib.request.urlretrieve(url, filename)\n",
        "            print(\"üì¶ Extracting dataset...\")\n",
        "            with tarfile.open(filename) as tar:\n",
        "                tar.extractall()\n",
        "        print(\"‚úÖ VOC2012 ready.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è VOC download failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# ==============================================================\n",
        "# SYNTHETIC FALLBACK GENERATOR\n",
        "# ==============================================================\n",
        "def generate_synthetic_dataset():\n",
        "    print(\"üé® Generating synthetic dataset...\")\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        os.makedirs(f\"{DATA_DIR}/{split}/images\", exist_ok=True)\n",
        "        os.makedirs(f\"{DATA_DIR}/{split}/masks\", exist_ok=True)\n",
        "\n",
        "    # simple color + shape masks\n",
        "    n_train = int(TRAIN_SPLIT * TARGET_SIZE)\n",
        "    n_val   = int(VAL_SPLIT * TARGET_SIZE)\n",
        "    n_test  = TARGET_SIZE - n_train - n_val\n",
        "    splits = {\"train\": n_train, \"val\": n_val, \"test\": n_test}\n",
        "\n",
        "    for split, count in splits.items():\n",
        "        for i in tqdm(range(count), desc=f\"Generating {split}\"):\n",
        "            img = np.random.randint(0, 255, (128, 128, 3), dtype=np.uint8)\n",
        "            mask = np.zeros((128, 128), dtype=np.uint8)\n",
        "\n",
        "            # draw a random rectangle mask\n",
        "            x1, y1 = np.random.randint(10, 64, size=2)\n",
        "            x2, y2 = np.random.randint(64, 118, size=2)\n",
        "            mask[y1:y2, x1:x2] = np.random.randint(1, 21)\n",
        "\n",
        "            Image.fromarray(img).save(f\"{DATA_DIR}/{split}/images/{split}_{i:04d}.jpg\")\n",
        "            Image.fromarray(mask).save(f\"{DATA_DIR}/{split}/masks/{split}_{i:04d}.png\")\n",
        "\n",
        "    print(\"‚úÖ Synthetic dataset ready!\")\n",
        "\n",
        "# ==============================================================\n",
        "# MAIN LOGIC\n",
        "# ==============================================================\n",
        "if download_voc():\n",
        "    # Load and subset VOC\n",
        "    trainval_path = os.path.join(VOC_ROOT, \"ImageSets\", \"Segmentation\", \"trainval.txt\")\n",
        "    with open(trainval_path) as f:\n",
        "        all_ids = [ln.strip() for ln in f.readlines()]\n",
        "\n",
        "    random.shuffle(all_ids)\n",
        "    ids_subset = all_ids[:TARGET_SIZE]\n",
        "\n",
        "    n_train = int(TRAIN_SPLIT * TARGET_SIZE)\n",
        "    n_val   = int(VAL_SPLIT * TARGET_SIZE)\n",
        "    train_ids = ids_subset[:n_train]\n",
        "    val_ids   = ids_subset[n_train:n_train+n_val]\n",
        "    test_ids  = ids_subset[n_train+n_val:]\n",
        "\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        os.makedirs(f\"{DATA_DIR}/{split}/images\", exist_ok=True)\n",
        "        os.makedirs(f\"{DATA_DIR}/{split}/masks\", exist_ok=True)\n",
        "\n",
        "    def copy_files(split, id_list):\n",
        "        for img_id in tqdm(id_list, desc=f\"Copying {split}\"):\n",
        "            src_img = os.path.join(VOC_ROOT, \"JPEGImages\", f\"{img_id}.jpg\")\n",
        "            src_mask = os.path.join(VOC_ROOT, \"SegmentationClass\", f\"{img_id}.png\")\n",
        "            dst_img = os.path.join(DATA_DIR, split, \"images\", f\"{img_id}.jpg\")\n",
        "            dst_mask = os.path.join(DATA_DIR, split, \"masks\", f\"{img_id}.png\")\n",
        "\n",
        "            if os.path.exists(src_img) and os.path.exists(src_mask):\n",
        "                shutil.copy(src_img, dst_img)\n",
        "                shutil.copy(src_mask, dst_mask)\n",
        "\n",
        "    copy_files(\"train\", train_ids)\n",
        "    copy_files(\"val\", val_ids)\n",
        "    copy_files(\"test\", test_ids)\n",
        "\n",
        "    print(\"‚úÖ VOC subset prepared successfully!\")\n",
        "else:\n",
        "    generate_synthetic_dataset()\n",
        "\n",
        "print(f\"üìÅ Dataset saved in: {os.path.abspath(DATA_DIR)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms + Dataloaders\n",
        "import os, cv2, torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '.png'))\n",
        "\n",
        "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Normalize mask to [0, num_classes)\n",
        "        mask = mask.astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            aug = self.transform(image=image, mask=mask)\n",
        "            image, mask = aug[\"image\"], aug[\"mask\"]\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "uluFMCuA5-Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model: U-Net (or DeepLabV3)\n",
        "# =============================================================\n",
        "# STEP 2: Create DataLoaders for train/val/test\n",
        "# =============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Transforms ---\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# --- Dataset paths ---\n",
        "BASE_DIR = \"data_voc_subset\"\n",
        "train_ds = VOCDataset(f\"{BASE_DIR}/train/images\", f\"{BASE_DIR}/train/masks\", transform=train_transform)\n",
        "val_ds   = VOCDataset(f\"{BASE_DIR}/val/images\", f\"{BASE_DIR}/val/masks\", transform=val_transform)\n",
        "test_ds  = VOCDataset(f\"{BASE_DIR}/test/images\", f\"{BASE_DIR}/test/masks\", transform=val_transform)\n",
        "\n",
        "# --- Dataloaders ---\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"‚úÖ Data ready: {len(train_ds)} train | {len(val_ds)} val | {len(test_ds)} test\")\n",
        "\n",
        "# --- Visualize one sample ---\n",
        "images, masks = next(iter(train_loader))\n",
        "img = images[0].permute(1, 2, 0).cpu().numpy()\n",
        "mask = masks[0].cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1); plt.imshow((img - img.min()) / (img.max() - img.min())); plt.title(\"Image\")\n",
        "plt.subplot(1,2,2); plt.imshow(mask, cmap='jet'); plt.title(\"Mask\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZqemnBJW89Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch -q"
      ],
      "metadata": {
        "id": "kmOX0Fvki0BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"üöÄ Using device:\", DEVICE)\n",
        "\n",
        "# --- Model ---\n",
        "# We‚Äôll use U-Net with a lightweight encoder (ResNet18)\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet18\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=21  # VOC has 20 classes + background\n",
        ").to(DEVICE)\n",
        "\n",
        "# --- Loss and optimizer ---\n",
        "loss_fn = smp.losses.DiceLoss(mode='multiclass')\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Training Loop ---\n",
        "EPOCHS = 5  # (you can increase later)\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = loss_fn(outputs, masks.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} done | Avg Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "3cp7Wz3y-t36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# STEP 4: Evaluate U-Net on Validation Set\n",
        "# =============================================================\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_iou_dice(preds, masks, num_classes=21):\n",
        "    ious, dices = [], []\n",
        "    preds = preds.cpu().numpy()\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    for cls in range(1, num_classes):  # skip background (0)\n",
        "        pred_cls = (preds == cls)\n",
        "        mask_cls = (masks == cls)\n",
        "        intersection = np.logical_and(pred_cls, mask_cls).sum()\n",
        "        union = np.logical_or(pred_cls, mask_cls).sum()\n",
        "        if union > 0:\n",
        "            iou = intersection / union\n",
        "            dice = 2 * intersection / (pred_cls.sum() + mask_cls.sum() + 1e-6)\n",
        "            ious.append(iou)\n",
        "            dices.append(dice)\n",
        "    return np.mean(ious), np.mean(dices)\n",
        "\n",
        "model.eval()\n",
        "iou_scores, dice_scores = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        iou, dice = compute_iou_dice(preds, masks)\n",
        "        iou_scores.append(iou)\n",
        "        dice_scores.append(dice)\n",
        "\n",
        "print(\"‚úÖ Validation Results:\")\n",
        "print(f\"Mean IoU:  {np.mean(iou_scores):.4f}\")\n",
        "print(f\"Mean Dice: {np.mean(dice_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "Tw1M6WzKfQMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    imgs, masks = next(iter(val_loader))\n",
        "    imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "    preds = torch.argmax(model(imgs), dim=1)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(3):\n",
        "    plt.subplot(3, 3, 3*i+1)\n",
        "    plt.imshow(imgs[i].permute(1,2,0).cpu())\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(3, 3, 3*i+2)\n",
        "    plt.imshow(masks[i].cpu(), cmap=\"nipy_spectral\")\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(3, 3, 3*i+3)\n",
        "    plt.imshow(preds[i].cpu(), cmap=\"nipy_spectral\")\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YMFBzdzIEb2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# STEP 6: Compare with Foundation Model (SAM)\n",
        "# =============================================================\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
        "!pip install opencv-python matplotlib -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "from PIL import Image\n",
        "\n",
        "# Load pretrained SAM\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "# Download SAM checkpoint if missing\n",
        "if not os.path.exists(sam_checkpoint):\n",
        "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device=DEVICE)\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "# Function to visualize SAM masks\n",
        "def show_sam_masks(image, masks, ax):\n",
        "    ax.imshow(image)\n",
        "    for m in masks:\n",
        "        seg = m[\"segmentation\"]\n",
        "        ax.imshow(seg, alpha=0.5)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Compare SAM vs U-Net on random test samples\n",
        "import random\n",
        "n_samples = 3\n",
        "indices = random.sample(range(len(test_ds)), n_samples)\n",
        "\n",
        "for i in indices:\n",
        "    img_path = os.path.join(test_ds.image_dir, test_ds.images[i])\n",
        "    mask_path = os.path.join(test_ds.mask_dir, test_ds.images[i].replace(\".jpg\", \".png\"))\n",
        "\n",
        "    image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "    gt_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # --- U-Net prediction ---\n",
        "    img_tensor = val_transform(image=image)[\"image\"].unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    # --- SAM zero-shot segmentation ---\n",
        "    sam_masks = mask_generator.generate(image)\n",
        "\n",
        "    # --- Display ---\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Input Image\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(gt_mask, cmap=\"jet\")\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    show_sam_masks(image, sam_masks, plt.gca())\n",
        "    plt.title(\"SAM Segmentation\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-4S83249D1Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# STEP 7: Extended Evaluation & Comparison Table\n",
        "# =============================================================\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "def compute_extended_metrics(model, dataloader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    all_preds, all_masks = [], []\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in tqdm(dataloader, desc=\"Evaluating (extended)\"):\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_masks.append(masks.cpu().numpy())\n",
        "    end = time.time()\n",
        "\n",
        "    preds = np.concatenate(all_preds).flatten()\n",
        "    masks = np.concatenate(all_masks).flatten()\n",
        "\n",
        "    precision = precision_score(masks, preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(masks, preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(masks, preds, average=\"macro\", zero_division=0)\n",
        "    acc = accuracy_score(masks, preds)\n",
        "    miou, mdice = compute_iou_dice(torch.tensor(preds), torch.tensor(masks))\n",
        "    fps = len(dataloader.dataset) / (end - start)\n",
        "\n",
        "    print(\"‚úÖ Extended Validation Metrics:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Mean IoU:  {miou:.4f}\")\n",
        "    print(f\"Mean Dice: {mdice:.4f}\")\n",
        "    print(f\"FPS:       {fps:.2f}\")\n",
        "\n",
        "    return precision, recall, f1, acc, miou, mdice, fps\n",
        "\n",
        "# Run evaluation for U-Net\n",
        "p, r, f1, acc, miou, mdice, fps = compute_extended_metrics(model, val_loader, DEVICE)\n",
        "\n",
        "# Create comparison table\n",
        "df_results = pd.DataFrame([\n",
        "    [\"U-Net (trained)\", \"Traditional\", miou, mdice, \"‚úÖ Yes\", \"Fast (GPU)\", \"Domain-specific\"],\n",
        "    [\"SAM (zero-shot)\", \"Foundation\", None, None, \"‚ùå No\", \"Moderate (CPU/GPU)\", \"Zero-shot general\"]\n",
        "], columns=[\"Model\", \"Type\", \"Mean IoU\", \"Mean Dice\", \"Training Required\", \"Inference Speed\", \"Generalization\"])\n",
        "\n",
        "display(df_results)"
      ],
      "metadata": {
        "id": "tIqOCy2qklPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}